{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Additional Modeling Pipelines\n",
    "We should also compare performance on the evaluation data (Buckeye test split) with other readily available phonetic transcription options, to determine whether fine-tuning your own model is worth the effort. \n",
    "The two options we consider here are: \n",
    "- [Allosaurus](https://github.com/xinjli/allosaurus) is a pre-trained universal phone recognizer that claims to recognize phones in more than 2000 languages. \n",
    "- [Whisper](https://openai.com/index/whisper/) is the state-of-the-art sequence-to-sequence speech recognition model released by OpenAI. Details about the different model releases are available at https://github.com/openai/whisper/blob/main/model-card.md. There are multilingual and English fine-tuned versions. We follow these models with grapheme to phoneme conversion using Epitran.\n",
    "- [excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k](https://huggingface.co/excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k) is a wav2vec2 model fine-tuned on TIMIT data. Because it uses the original TIMIT phonemes, we post-process using [phonecodes](https://pypi.org/project/phonecodes/) to convert predictions to IPA. \n",
    "\n",
    "These evaluations only need to be run and computed once. \n",
    "\n",
    "## Additional installation step for Epitran\n",
    "To use Epitran for English, you also need to install https://github.com/festvox/flite. See the Epitran note at https://github.com/dmort27/epitran?tab=readme-ov-file#installation-of-flite-for-english-g2p.  I installed Flite on my mac:\n",
    "\n",
    "```bash\n",
    "$ git clone http://github.com/festvox/flite\n",
    "$ cd flite\n",
    "$ ./configure && make\n",
    "$ sudo make install\n",
    "$ cd testsuite\n",
    "$ make lex_lookup\n",
    "$ sudo cp lex_lookup /usr/local/bin\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/panphon/featuretable.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import allosaurus.app\n",
    "import allosaurus.bin.download_model\n",
    "import datasets\n",
    "import epitran\n",
    "from phonecodes import phonecodes\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multipa.data_utils import load_buckeye_split, clean_text\n",
    "from multipa.evaluation import ModelEvaluator, preprocess_test_data, write_detailed_prediction_results, DETAILED_PREDICTIONS_CSV_SUFFIX, PREDICTION_KEY, clean_model_name\n",
    "\n",
    "VERBOSE_RESULTS_DIR = Path(\"../data/evaluation_results/detailed_predictions\")\n",
    "AGGREGATE_METRICS_CSV = Path(\"../data/evaluation_results/aggregate_metrics/epitran_allosaurus_eval.csv\")\n",
    "EDIT_DIST_DIR = Path(\"../data/evaluation_results/edit_distances/\")\n",
    "\n",
    "IS_REMOVE_SPACES = True\n",
    "NUM_PROC = 8 # For HuggingFace dataset map and filter\n",
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allosaurus_predict(test_dataset, model=\"eng2102\", phone_inventory=\"ipa\"):\n",
    "    print(\"Evaluating allosaurus. Model:\", model, \"Phone inventory:\", phone_inventory)\n",
    "    model_predictions = []\n",
    "    model = allosaurus.app.read_recognizer(model)\n",
    "    start = time.time()\n",
    "    for audio in tqdm(test_dataset[\"audio\"]):\n",
    "        prediction = model.recognize(audio[\"path\"], phone_inventory)\n",
    "        model_predictions.append({PREDICTION_KEY: prediction})\n",
    "    end = time.time()\n",
    "    print(\"Eval time in seconds:\", end-start)\n",
    "    predictions_dataset = datasets.Dataset.from_list(model_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: clean_text(x, text_key=PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n",
    "\n",
    "def hf_model_to_epitran_predict(model_name, test_dataset):\n",
    "    print(\"Building pipeline and downloading model\")\n",
    "    if model_name.endswith(\".en\"):\n",
    "        pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    else:\n",
    "        pipe = transformers.pipeline(\n",
    "            \"automatic-speech-recognition\", model=model_name, device=DEVICE, generate_kwargs={\"language\": \"english\"}\n",
    "        )\n",
    "    print(\"Predicting with\", model_name)\n",
    "    start = time.time()\n",
    "    orthography_predictions = [d[\"text\"] for d in pipe(test_dataset[\"audio\"])]\n",
    "    epi = epitran.Epitran('eng-Latn')\n",
    "    print(\"Transliterating with Epitran\")\n",
    "    ipa_predictions = []\n",
    "    for pred in tqdm(orthography_predictions):\n",
    "        result = epi.transliterate(pred)\n",
    "        ipa_predictions.append({PREDICTION_KEY: result})\n",
    "    end = time.time()\n",
    "    print(\"Eval time in seconds:\", end-start)\n",
    "    predictions_dataset = datasets.Dataset.from_list(ipa_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: clean_text(x, text_key=PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n",
    "\n",
    "\n",
    "def phonecodes_convert_batch(batch: dict, in_code=\"timit\", out_code=\"ipa\"): \n",
    "    \"\"\"\n",
    "    Phonecodes conversion that operates on Datasets\n",
    "    \"\"\"\n",
    "    in_str = batch[in_code]\n",
    "    conversion = phonecodes.convert(in_str, in_code, out_code)\n",
    "    batch[out_code] = conversion\n",
    "    return batch\n",
    "\n",
    "\n",
    "def hf_to_phonecodes(\n",
    "    test_dataset, \n",
    "    model_name=\"excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k\", \n",
    "    in_code=\"timit\", out_code=\"ipa\"):\n",
    "    pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    predictions_dataset = datasets.Dataset.from_list(\n",
    "        [{in_code:d[\"text\"]} for d in pipe(test_dataset[\"audio\"])]\n",
    "    )\n",
    "    # convert to ipa\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: phonecodes_convert_batch(x, in_code, out_code), num_proc=NUM_PROC\n",
    "    )\n",
    "    # clean prediction output\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: clean_text(x, text_key=out_code, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    predictions_dataset = predictions_dataset.rename_column(out_code, PREDICTION_KEY)\n",
    "    predictions_dataset = predictions_dataset.rename_column(in_code, f\"{in_code}_{PREDICTION_KEY}\")\n",
    "    return predictions_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc65e4f6dbb4390813157e7143a8f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc0916ef1a64d6c8dcc1d29d732ec8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd917fb33f374e46896bc59a603d355a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preview\n",
      "Dataset({\n",
      "    features: ['utterance_id', 'duration', 'buckeye_transcript', 'text', 'ipa', 'speaker_id', 'speaker_gender', 'speaker_age_range', 'interviewer_gender', 'file_path', 'audio'],\n",
      "    num_rows: 5079\n",
      "})\n",
      "{'utterance_id': 's2501a_Utt0', 'duration': 0.925981, 'buckeye_transcript': 'f ao r f ay v', 'text': 'four five', 'ipa': 'f ɔ ɹ f aɪ v', 'speaker_id': 'S25', 'speaker_gender': 'f', 'speaker_age_range': 'o', 'interviewer_gender': 'm', 'file_path': 'data/buckeye/test/s2501a_Utt0.wav', 'audio': {'bytes': None, 'path': '/work/pi_vcpartridge_umass_edu/multipa/data/buckeye/test/s2501a_Utt0.wav'}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb214eced0947a2a5e579280d3673c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/5079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90f1301dd034481b02b2491fbd249bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/5079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d148833ee846c3ad884290cd19cedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/5079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data with speech transcriptions\n",
      "Dataset({\n",
      "    features: ['utterance_id', 'duration', 'buckeye_transcript', 'text', 'ipa', 'speaker_id', 'speaker_gender', 'speaker_age_range', 'interviewer_gender', 'file_path', 'audio'],\n",
      "    num_rows: 5079\n",
      "})\n",
      "{'utterance_id': 's2501a_Utt0', 'duration': 0.925981, 'buckeye_transcript': 'f ao r f ay v', 'text': 'four five', 'ipa': 'fɔɹfaɪv', 'speaker_id': 'S25', 'speaker_gender': 'f', 'speaker_age_range': 'o', 'interviewer_gender': 'm', 'file_path': 'data/buckeye/test/s2501a_Utt0.wav', 'audio': {'path': '/work/pi_vcpartridge_umass_edu/multipa/data/buckeye/test/s2501a_Utt0.wav', 'array': array([-0.00997925, -0.01052856, -0.00958252, ...,  0.00085449,\n",
      "        0.00061035,  0.00042725], shape=(14816,)), 'sampling_rate': 16000}}\n",
      "Test data without speech\n",
      "Dataset({\n",
      "    features: ['utterance_id', 'duration', 'buckeye_transcript', 'text', 'ipa', 'speaker_id', 'speaker_gender', 'speaker_age_range', 'interviewer_gender', 'file_path', 'audio'],\n",
      "    num_rows: 0\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "input_data = load_buckeye_split(\"../data/buckeye\", \"test\")\n",
    "# Snippet of transcriptions\n",
    "# Note that there don't appear to be any non-empty transcriptions,\n",
    "# so this notebook skips looking at hallucinations\n",
    "print(\"Data Preview\")\n",
    "print(input_data)\n",
    "print(input_data[0])\n",
    "\n",
    "non_empty_test_data, empty_test_data = preprocess_test_data(input_data, is_remove_space=True, num_proc=NUM_PROC)\n",
    "\n",
    "print(\"Test data with speech transcriptions\")\n",
    "print(non_empty_test_data)\n",
    "print(non_empty_test_data[0])\n",
    "print(\"Test data without speech\")\n",
    "print(empty_test_data)\n",
    "\n",
    "model_evaluator = ModelEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"openai/whisper-large-v3-turbo\",\n",
    "    #\"openai/whisper-large-v3\",\n",
    "    \"openai/whisper-medium.en\",\n",
    "]\n",
    "for m in models:\n",
    "    # Epitran\n",
    "    epitran_predictions = hf_model_to_epitran_predict(m, non_empty_test_data)\n",
    "    model_name = f\"{m}_to_epitran\".replace(\"/\", \"_\")\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, epitran_predictions[PREDICTION_KEY], non_empty_test_data[\"ipa\"]\n",
    "    )\n",
    "    write_detailed_prediction_results(VERBOSE_RESULTS_DIR, model_name, non_empty_test_data, epitran_predictions, metrics)\n",
    "    model_evaluator.write_edit_distance_results(model_name,EDIT_DIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and phone inventory to test\n",
    "# allosaurus_models = [\"uni2005\", \"eng2102\"]\n",
    "# phone_inventory = [\"ipa\", \"eng\"]\n",
    "\n",
    "allosaurus_models = [\"eng2102\"]\n",
    "phone_inventory = [\"eng\"]\n",
    "\n",
    "# Download models\n",
    "for m in allosaurus_models:\n",
    "    allosaurus.bin.download_model.download_model(m)\n",
    "\n",
    "# Predict and check against gold standard\n",
    "for model, pi in itertools.product(allosaurus_models, phone_inventory):\n",
    "    model_predictions = allosaurus_predict(non_empty_test_data, model, pi)\n",
    "    model_name = f\"allosaurus_{model}_{pi}\"\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(model_name, model_predictions[PREDICTION_KEY], non_empty_test_data[\"ipa\"])\n",
    "    write_detailed_prediction_results(\n",
    "        VERBOSE_RESULTS_DIR, model_name, non_empty_test_data, model_predictions, metrics\n",
    "    )\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "hf_to_phonecodes_models = [(\"excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k\", \"timit\", \"ipa\")]\n",
    "\n",
    "for model_name, in_code, out_code in hf_to_phonecodes_models: \n",
    "    model_predictions = hf_to_phonecodes(non_empty_test_data, model_name, in_code, out_code)\n",
    "    print(model_predictions)\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, \n",
    "        model_predictions[PREDICTION_KEY], \n",
    "        non_empty_test_data[\"ipa\"])\n",
    "    write_detailed_prediction_results(\n",
    "        VERBOSE_RESULTS_DIR, clean_model_name(model_name), non_empty_test_data, model_predictions, metrics\n",
    "    )\n",
    "    write_edit_distance_results(model_name, EDIT_DIST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all results to file for comparison\n",
    "model_evaluator.to_csv(AGGREGATE_METRICS_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
