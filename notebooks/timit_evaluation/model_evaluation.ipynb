{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16409b39",
   "metadata": {},
   "source": [
    "# TIMIT Evaluation \n",
    "\n",
    "Runs evaluation scripts on the TIMIT corpus to get phone error rates and edit distances for TIMIT (unseen data) for the following models:\n",
    "- Ours models that were fine-tuned on the Buckeye corpus\n",
    "- C. Taguchi Model\n",
    "- Allosaraus Model\n",
    "- Whispter to Epitran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e93c94",
   "metadata": {},
   "source": [
    "### Additional installation step for Epitran\n",
    "\n",
    "```bash\n",
    "$ git clone http://github.com/festvox/flite\n",
    "$ cd flite\n",
    "$ ./configure && make\n",
    "$ sudo make install\n",
    "$ cd testsuite\n",
    "$ make lex_lookup\n",
    "$ sudo cp lex_lookup /usr/local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87390d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/panphon/featuretable.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import allosaurus.app\n",
    "import allosaurus.bin.download_model\n",
    "import datasets\n",
    "import epitran\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from phonecodes import phonecodes\n",
    "import soundfile as sf\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multipa.data_utils\n",
    "import multipa.evaluation\n",
    "\n",
    "\n",
    "DEVICE = 0  # -1 for CPU, or set GPU index if available\n",
    "\n",
    "# Paths For TIMIT Database and TIMIT IPA\n",
    "# timit_data_dir = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/COMPLETE\")\n",
    "# transcriptions_path = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/complete_ipa.csv\")\n",
    "timit_data_dir = Path(\"../../data/TIMIT Dataset/COMPLETE\")\n",
    "transcriptions_path = Path(\"../../data/TIMIT Dataset/complete_ipa.csv\")\n",
    "\n",
    "# HuggingFace Models Evaluating\n",
    "our_model = \"ginic/full_dataset_train_3_wav2vec2-large-xlsr-53-buckeye-ipa\"\n",
    "taguchi_1k = \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\"\n",
    "\n",
    "# Set up results directories\n",
    "RESULTS_DIR =Path(\"../../data/timit_results\")\n",
    "VERBOSE_RESULTS_DIR = RESULTS_DIR / \"detailed_predictions\"\n",
    "AGGREGATE_METRICS_CSV = RESULTS_DIR / \"aggregate_metrics/all_models_eval.csv\"\n",
    "EDIT_DIST_DIR = RESULTS_DIR / \"edit_distances\"\n",
    "VERBOSE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGGREGATE_METRICS_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "EDIT_DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing options\n",
    "IS_REMOVE_SPACES = True\n",
    "NUM_PROC = 8 # Number of processes for HuggingFace dataset map and filter\n",
    "\n",
    "# Computes and stores by-model performance metrics\n",
    "model_evaluator = multipa.evaluation.ModelEvaluator()\n",
    "\n",
    "evaluated_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8f0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_timit_gold_standard_transcriptions(transcriptions_path):\n",
    "    \"\"\"Returns a dictionary of {\"audio_filename\" -> {\"ipa_transcription\": transcription, \"filename\": original_filename}}\"\"\"\n",
    "    gold_standard_df = pd.read_csv(transcriptions_path)\n",
    "    gold_standard_df[\"filename\"] = gold_standard_df[\"audio_filename\"].str.lower()\n",
    "    gold_standard_df.set_index(\"filename\", inplace=True)\n",
    "    return gold_standard_df.to_dict(\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62ccc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WAV files found: 6300\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR1/FAKS0/SA1.WAV'}, 'filename': '/complete/dr1/faks0/sa1.wav', 'ipa': ' ʃ i ɦ æ d j ɝ d ɑ ɹ k s u ɾ ɪ ŋ g ɹ i s i w ɑ ʃ  w ɑ ɾ ɝ ʔ ɔ l j i ɚ '}\n"
     ]
    }
   ],
   "source": [
    "# Load TIMIT audio as a HuggingFace dataset with audio and gold standard transcriptions together\n",
    "# This loads TIMIT as a Dataset with the same columns as the Buckeye corpus we've been working with\n",
    "gold_standard_transcriptions = read_timit_gold_standard_transcriptions(transcriptions_path)\n",
    "\n",
    "timit_wavs = [p for p in timit_data_dir.rglob(\"*\") if p.suffix.lower() == \".wav\"]\n",
    "print(\"Total WAV files found:\", len(timit_wavs))\n",
    "data = []\n",
    "\n",
    "for p in timit_wavs:\n",
    "    clean_filename = \"/\" + str(p.relative_to(timit_data_dir.parent)).lower()\n",
    "    ipa_transcription = gold_standard_transcriptions[clean_filename][\"ipa_transcription\"]\n",
    "\n",
    "    entry = {\n",
    "        \"audio\": {\"path\": str(p)},\n",
    "        \"filename\": clean_filename,\n",
    "        \"ipa\":ipa_transcription\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "audio_dataset = datasets.Dataset.from_list(data)\n",
    "print(audio_dataset)\n",
    "print(audio_dataset[0])\n",
    "\n",
    "# TODO: Evaluate on the whole dataset\n",
    "# Test with a small subset if wanted\n",
    "# audio_subset = audio_dataset.take(10)\n",
    "audio_subset = audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14976a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b78fd3de8454685b0f956a0e157e0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aedb3dc484b47fd86355236c591cf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425dc625a5394bca91a90c9aa1983afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio with speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR1/FAKS0/SA1.WAV', 'array': array([9.15527344e-05, 1.52587891e-04, 6.10351562e-05, ...,\n",
      "       2.44140625e-04, 3.05175781e-04, 2.13623047e-04], shape=(63488,)), 'sampling_rate': 16000}, 'filename': '/complete/dr1/faks0/sa1.wav', 'ipa': 'ʃiɦædjɝdɑɹksuɾɪŋgɹisiwɑʃwɑɾɝʔɔljiɚ'}\n",
      "Audio without speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 0\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Sample audio correctly and preprocess transcriptions to remove whitepsace\n",
    "audio_subset, audio_without_speech = multipa.evaluation.preprocess_test_data(audio_subset,\n",
    "    is_remove_space=IS_REMOVE_SPACES, num_proc=NUM_PROC)\n",
    "print(\"Audio with speech transcriptions\")\n",
    "print(audio_subset)\n",
    "print(audio_subset[0])\n",
    "\n",
    "# Sanity check that there's no audio without transcirptions\n",
    "print(\"Audio without speech transcriptions\")\n",
    "print(audio_without_speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82806879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allosaurus_predict(test_dataset, model=\"eng2102\", phone_inventory=\"ipa\"):\n",
    "    print(\"Evaluating allosaurus. Model:\", model, \"Phone inventory:\", phone_inventory)\n",
    "    model_predictions = []\n",
    "    recog = allosaurus.app.read_recognizer(model)\n",
    "    for audio in tqdm(test_dataset[\"audio\"]):\n",
    "        wav_path = audio[\"path\"]\n",
    "        data, sr = sf.read(wav_path)\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\") as tmp:\n",
    "            sf.write(tmp.name, data, sr, format=\"WAV\", subtype=\"PCM_16\")\n",
    "            prediction = recog.recognize(tmp.name, phone_inventory)\n",
    "        #prediction = model.recognize(audio[\"path\"], phone_inventory)\n",
    "            model_predictions.append({multipa.evaluation.PREDICTION_KEY: prediction})\n",
    "    predictions_dataset = datasets.Dataset.from_list(model_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: multipa.data_utils.clean_text(x, text_key=multipa.evaluation.PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n",
    "\n",
    "def hf_model_to_epitran_predict(model_name, test_dataset):\n",
    "    print(\"Building pipeline and downloading model\")\n",
    "    if model_name.endswith(\".en\"):\n",
    "        pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    else:\n",
    "        pipe = transformers.pipeline(\n",
    "            \"automatic-speech-recognition\", model=model_name, generate_kwargs={\"language\": \"english\"}, device=DEVICE\n",
    "        )\n",
    "    print(\"Predicting with\", model_name)\n",
    "    orthography_predictions = [d[\"text\"] for d in pipe(test_dataset[\"audio\"])]\n",
    "    epi = epitran.Epitran('eng-Latn')\n",
    "    print(\"Transliterating with Epitran\")\n",
    "    ipa_predictions = []\n",
    "    for pred in tqdm(orthography_predictions):\n",
    "        result = epi.transliterate(pred)\n",
    "        ipa_predictions.append({multipa.evaluation.PREDICTION_KEY: result})\n",
    "    predictions_dataset = datasets.Dataset.from_list(ipa_predictions)\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: multipa.data_utils.clean_text(x, text_key=multipa.evaluation.PREDICTION_KEY, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    return predictions_dataset\n",
    "\n",
    "def phonecodes_convert_batch(batch: dict, in_code=\"timit\", out_code=\"ipa\"): \n",
    "    \"\"\"\n",
    "    Phonecodes conversion that operates on Datasets\n",
    "    \"\"\"\n",
    "    in_str = batch[in_code]\n",
    "    conversion = phonecodes.convert(in_str, in_code, out_code)\n",
    "    batch[out_code] = conversion\n",
    "    return batch\n",
    "\n",
    "\n",
    "def hf_to_phonecodes(\n",
    "    test_dataset, \n",
    "    model_name=\"excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k\", \n",
    "    in_code=\"timit\", out_code=\"ipa\"):\n",
    "    pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    predictions_dataset = datasets.Dataset.from_list(\n",
    "        [{in_code:d[\"text\"]} for d in pipe(test_dataset[\"audio\"])]\n",
    "    )\n",
    "    # convert to ipa\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: phonecodes_convert_batch(x, in_code, out_code), num_proc=NUM_PROC\n",
    "    )\n",
    "    # clean prediction output\n",
    "    predictions_dataset = predictions_dataset.map(\n",
    "        lambda x: multipa.data_utils.clean_text(x, text_key=out_code, is_remove_space=IS_REMOVE_SPACES), num_proc=NUM_PROC\n",
    "    )\n",
    "    predictions_dataset = predictions_dataset.rename_column(out_code, multipa.evaluation.PREDICTION_KEY)\n",
    "    predictions_dataset = predictions_dataset.rename_column(in_code, f\"{in_code}_{multipa.evaluation.PREDICTION_KEY}\")\n",
    "    return predictions_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276afb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allosaurus inference and metrics compute\n",
    "allosaurus_model = \"eng2102\"\n",
    "phone_inventory = \"eng\"\n",
    "allosaurus_model_name = f\"allosaurus_{allosaurus_model}_{phone_inventory}\"\n",
    "allosaurus.bin.download_model.download_model(allosaurus_model)\n",
    "allosaurus_predictions = allosaurus_predict(audio_subset, model=allosaurus_model, phone_inventory=phone_inventory)\n",
    "allosaurus_metrics = model_evaluator.eval_non_empty_transcriptions(allosaurus_model_name,\n",
    "    allosaurus_predictions[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "# Write prediction details and edit distances\n",
    "model_evaluator.write_edit_distance_results(allosaurus_model_name, EDIT_DIST_DIR)\n",
    "multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, allosaurus_model_name, audio_subset, allosaurus_predictions, allosaurus_metrics)\n",
    "\n",
    "# Save model results for later\n",
    "print(\"Done evaluating Allosaurus\")\n",
    "evaluated_models.append(allosaurus_model_name)\n",
    "full_analysis_dataset = audio_subset.add_column(allosaurus_model_name, allosaurus_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace model inference and evaluation\n",
    "models = [our_model, taguchi_1k]\n",
    "for model_name in models:\n",
    "    clean_model_name = multipa.evaluation.clean_model_name(model_name)\n",
    "    print(f\"Running ASR for model: {model_name}\")\n",
    "    asr_pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    predictions_dataset = multipa.evaluation.get_clean_predictions(audio_subset, asr_pipe,\n",
    "        num_proc=NUM_PROC, is_remove_space=IS_REMOVE_SPACES)\n",
    "\n",
    "    # Compute all metrics\n",
    "    model_metrics = model_evaluator.eval_non_empty_transcriptions(model_name,\n",
    "        predictions_dataset[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "    # Write prediction details and edit distances\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, clean_model_name, audio_subset, predictions_dataset, model_metrics)\n",
    "\n",
    "    print(\"Done evaluating\", model_name)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = full_analysis_dataset.add_column(name=model_name, column=predictions_dataset[multipa.evaluation.PREDICTION_KEY])\n",
    "    print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orthographic to epitran models\n",
    "models = [\n",
    "    \"openai/whisper-large-v3-turbo\",\n",
    "    # \"openai/whisper-large-v3\",\n",
    "    \"openai/whisper-medium.en\",\n",
    "]\n",
    "for m in models:\n",
    "    model_name = f\"{m}_to_epitran\".replace(\"/\", \"_\")\n",
    "    print(\"Evaulating\", model_name)\n",
    "    epitran_predictions = hf_model_to_epitran_predict(m, audio_subset)\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, epitran_predictions[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"]\n",
    "    )\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, model_name, audio_subset, epitran_predictions, metrics)\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    print(\"Done evaluating\", model_name)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = full_analysis_dataset.add_column(name=model_name, column=epitran_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "    print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd9b7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d6a1b056dc4df48ce7ef7c6c9aa3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee3cef3ed9e48b69f5da697fa57b982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['timit_prediction', 'prediction'],\n",
      "    num_rows: 6300\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a77f05bc664c7a8bb89731eaf85665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff27765e2f4e4e37b35db30d3342c2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_to_phonecodes_models = [(\"excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k\", \"timit\", \"ipa\")]\n",
    "\n",
    "for model_name, in_code, out_code in hf_to_phonecodes_models: \n",
    "    model_predictions = hf_to_phonecodes(audio_subset, model_name, in_code, out_code)\n",
    "    print(model_predictions)\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, \n",
    "        model_predictions[multipa.evaluation.PREDICTION_KEY], \n",
    "        audio_subset[\"ipa\"])\n",
    "    multipa.evaluation.write_detailed_prediction_results(\n",
    "        VERBOSE_RESULTS_DIR, multipa.evaluation.clean_model_name(model_name), audio_subset, model_predictions, metrics\n",
    "    )\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    evaluated_models.append(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4281ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all results to file for comparison\n",
    "model_evaluator.to_csv(AGGREGATE_METRICS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e6e7994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These models were evaluated: ['excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k']\n",
      "Dataset snippet for full anslysis:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'full_analysis_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThese models were evaluated:\u001b[39m\u001b[33m\"\u001b[39m, evaluated_models)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset snippet for full anslysis:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfull_analysis_dataset\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(full_analysis_dataset[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'full_analysis_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"These models were evaluated:\", evaluated_models)\n",
    "print(\"Dataset snippet for full anslysis:\")\n",
    "print(full_analysis_dataset)\n",
    "print(full_analysis_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = full_analysis_dataset.to_pandas()\n",
    "print(\"predictions_df snippet\")\n",
    "print(predictions_df.head())\n",
    "\n",
    "full_comparison_df = predictions_df.drop(\n",
    "    columns=[\"audio\"]\n",
    "    )\n",
    "\n",
    "print(\"full_comparison_df snippet\")\n",
    "print(full_comparison_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_col = \"ipa\"\n",
    "model_names = evaluated_models\n",
    "model_eval = multipa.evaluation.ModelEvaluator()\n",
    "\n",
    "def extract_dialect(path_str):\n",
    "    path = Path(path_str)\n",
    "    parts = [p for p in path.parts if p.lower().startswith(\"dr\")]\n",
    "    return parts[0].upper() if parts else \"UNKNOWN\"\n",
    "\n",
    "full_comparison_df[\"dialect\"] = full_comparison_df[\"filename\"].apply(extract_dialect)\n",
    "print(\"Dialect groups found:\", full_comparison_df[\"dialect\"].unique())\n",
    "\n",
    "summary_data = {}\n",
    "dialect_results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "    predictions = full_comparison_df[model_name].tolist()\n",
    "    references = full_comparison_df[gold_col].tolist()\n",
    "\n",
    "    metrics = model_eval.eval_non_empty_transcriptions(model_name, predictions, references)\n",
    "\n",
    "    for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "        col_name = f\"{metric_name} VS {model_name}\"\n",
    "        full_comparison_df[col_name] = metrics[metric_name]\n",
    "\n",
    "    summary_data[model_name] = {\n",
    "        metric_name: float(np.mean(metrics[metric_name]))\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]\n",
    "    }\n",
    "\n",
    "    for dialect, df_group in full_comparison_df.groupby(\"dialect\"):\n",
    "        result_row = {\n",
    "            \"dialect\": dialect,\n",
    "            \"model\": model_name,\n",
    "        }\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "            col_name = f\"{metric_name} VS {model_name}\"\n",
    "            result_row[metric_name] = df_group[col_name].mean()\n",
    "        dialect_results.append(result_row)\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).T\n",
    "summary_df = summary_df[[\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]]\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df = summary_df.rename(columns={\"index\": \"model\"})\n",
    "summary_df.to_csv(\"timit_model_evaluation_summary.csv\", index=False)\n",
    "print(\"Average evaluation metrics per model saved to timit_model_evaluation_summary.csv\")\n",
    "\n",
    "\n",
    "dialect_summary_df = pd.DataFrame(dialect_results)\n",
    "dialect_summary_df.to_csv(\"timit_dialect_model_comparison.csv\", index=False)\n",
    "print(\"Dialect evaluation complete. Results saved to timit_dialect_model_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
